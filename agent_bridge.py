#!/usr/bin/env python3
"""
Agent Bridge - Connects Orchestrator to real AI agents
Integrates TestRun's proven agent execution with Orchestrator's framework
"""

import subprocess
import os
import json
import asyncio
from typing import Dict, Any, Optional, List
from datetime import datetime
from pathlib import Path
import aiofiles
from abc import ABC, abstractmethod


class AgentConnector(ABC):
    """Abstract base class for agent connectors"""
    
    def __init__(self):
        self.log_callback = None
    
    def set_log_callback(self, callback):
        """Set callback function for agent logging"""
        self.log_callback = callback
    
    def log(self, level: str, message: str):
        """Log message with callback if set"""
        if self.log_callback:
            self.log_callback(level, message, self.__class__.__name__.replace('Connector', '').lower())
    
    @abstractmethod
    async def execute(self, prompt: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """Execute task with agent and return results"""
        pass
    
    @abstractmethod
    def get_capabilities(self) -> List[str]:
        """Return agent capabilities"""
        pass


class ClaudeConnector(AgentConnector):
    """Connect to Claude via Anthropic SDK"""
    
    def __init__(self, project_path: str):
        super().__init__()
        self.project_path = project_path
    
    async def execute(self, prompt: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """Execute Claude for analysis and strategy"""
        self.log("info", "ðŸŽ¯ Starting analysis task...")
        self.log("info", f"ðŸ“ Prompt: {prompt[:100]}...")
        print(f"ðŸŽ¯ CLAUDE: Starting analysis task...")
        print(f"ðŸ“ CLAUDE: Prompt: {prompt[:100]}...")
        
        try:
            # Use the Anthropic Python SDK
            import anthropic
            
            self.log("info", "ðŸ”‘ Checking API key...")
            print("ðŸ”‘ CLAUDE: Checking API key...")
            # Get API key from environment
            api_key = os.getenv("ANTHROPIC_API_KEY")
            if not api_key:
                self.log("warning", "âš ï¸ No API key in environment, checking settings...")
                print("âš ï¸  CLAUDE: No API key in environment, checking settings...")
                # Try to load from saved settings
                settings_file = Path("settings.json")
                if settings_file.exists():
                    with open(settings_file, "r") as f:
                        settings = json.load(f)
                        api_key = settings.get("anthropic_key")
            
            if not api_key:
                self.log("error", "âŒ No API key found!")
                print("âŒ CLAUDE: No API key found!")
                raise Exception("No Anthropic API key found")
            
            self.log("info", "âœ… API key found, initializing client...")
            print("âœ… CLAUDE: API key found, initializing client...")
            # Initialize client
            client = anthropic.Anthropic(api_key=api_key)
            
            # Enhance weak prompts first
            enhanced_prompt = self._enhance_prompt(prompt)
            
            # Build the actual prompt for Claude
            full_prompt = f"""You are an AI architect analyzing requirements for implementation.

User requirement: {enhanced_prompt}

Please analyze this requirement and provide:
1. A clear and detailed description of what needs to be built
2. The main components/files needed with their purposes
3. The technology stack to use (prefer web technologies for visual outputs)
4. Any key decisions or considerations
5. Specific implementation details to guide the code generator

For visual/graphic requests, recommend HTML/CSS/SVG solutions unless specifically asked for other formats.

Keep your response structured and actionable."""

            self.log("info", "ðŸš€ Making API call to Claude-3-Sonnet...")
            print("ðŸš€ CLAUDE: Making API call to Claude-3-Sonnet...")
            # Make API call
            message = client.messages.create(
                model="claude-3-sonnet-20240229",
                max_tokens=1000,
                temperature=0,
                messages=[{
                    "role": "user",
                    "content": full_prompt
                }]
            )
            
            self.log("success", f"âœ… Received response ({len(message.content[0].text)} chars)")
            self.log("info", f"ðŸ“„ Response preview: {message.content[0].text[:150]}...")
            print(f"âœ… CLAUDE: Received response ({len(message.content[0].text)} chars)")
            print(f"ðŸ“„ CLAUDE: Response preview: {message.content[0].text[:150]}...")
            
            response = message.content[0].text
            
            return {
                "success": True,
                "response": response,
                "agent": "claude",
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "agent": "claude"
            }
    
    def _enhance_prompt(self, prompt: str) -> str:
        """Enhance weak or unclear prompts with better context"""
        prompt_lower = prompt.lower()
        
        # Common weak prompts and their enhancements
        enhancements = {
            "make a webpage": "Create a modern, responsive webpage with HTML5, CSS3, and JavaScript",
            "build a site": "Develop a complete website with modern UI/UX design",
            "make a graphic": "Create a visual graphic using web technologies (SVG or Canvas)",
            "draw": "Create a drawing/illustration using HTML5 Canvas or SVG",
            "make an app": "Build a web application with user interface and functionality",
            "create api": "Develop a RESTful API with proper endpoints, authentication, and documentation"
        }
        
        # Check for vague prompts
        if len(prompt.split()) < 5:  # Very short prompts
            for key, enhancement in enhancements.items():
                if key in prompt_lower:
                    # Enhance but keep the specific request
                    specifics = prompt_lower.replace(key, "").strip()
                    if specifics:
                        return f"{enhancement} {specifics}"
                    return enhancement
        
        # Enhance graphics requests
        graphics_words = ['graphic', 'image', 'picture', 'draw', 'visual', 'display']
        if any(word in prompt_lower for word in graphics_words):
            if 'of' in prompt_lower:
                # Extract what to draw
                parts = prompt_lower.split('of', 1)
                if len(parts) > 1:
                    subject = parts[1].strip()
                    return f"Create an interactive web-based graphic visualization of {subject} using modern HTML5/CSS3/SVG technologies with animations and proper styling"
        
        return prompt
    
    def get_capabilities(self) -> List[str]:
        return [
            "architecture_design",
            "requirement_analysis",
            "strategic_planning", 
            "risk_assessment",
            "system_design"
        ]


class CodexConnector(AgentConnector):
    """Connect to OpenAI Codex for implementation"""
    
    def __init__(self, api_key: Optional[str] = None):
        super().__init__()
        self.api_key = api_key or os.getenv("OPENAI_API_KEY")
        self._client = None
    
    def _get_client(self):
        """Lazy load OpenAI client"""
        if not self._client and self.api_key:
            try:
                from openai import OpenAI
                self._client = OpenAI(api_key=self.api_key)
            except ImportError:
                pass
        return self._client
    
    async def execute(self, prompt: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """Execute Codex for code generation"""
        self.log("info", "ðŸ’» Starting code generation...")
        self.log("info", f"ðŸ“ Task: {prompt[:100]}...")
        print(f"ðŸ’» CODEX: Starting code generation...")
        print(f"ðŸ“ CODEX: Task: {prompt[:100]}...")
        
        client = self._get_client()
        
        if not client:
            self.log("warning", "âš ï¸ No OpenAI client available, using simulation...")
            print("âš ï¸  CODEX: No OpenAI client available, using simulation...")
            # Fallback to simulation
            return await self._simulate_codex(prompt, context)
        
        self.log("info", "âœ… OpenAI client ready, generating code...")
        print("âœ… CODEX: OpenAI client ready, generating code...")
        
        try:
            self.log("info", "ðŸš€ Making API call to GPT-3.5-Turbo-Instruct...")
            print("ðŸš€ CODEX: Making API call to GPT-3.5-Turbo-Instruct...")
            # Real API call
            response = await asyncio.to_thread(
                client.completions.create,
                model="gpt-3.5-turbo-instruct",
                prompt=self._format_prompt(prompt, context),
                max_tokens=1000,
                temperature=0.7
            )
            
            self.log("success", f"âœ… Code generated ({len(response.choices[0].text)} chars)")
            self.log("info", f"ðŸ“„ Code preview: {response.choices[0].text[:150]}...")
            print(f"âœ… CODEX: Code generated ({len(response.choices[0].text)} chars)")
            print(f"ðŸ“„ CODEX: Code preview: {response.choices[0].text[:150]}...")
            
            return {
                "success": True,
                "response": response.choices[0].text,
                "agent": "codex",
                "model": response.model,
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            self.log("error", f"âŒ API call failed: {e}")
            self.log("warning", "ðŸ”„ Falling back to simulation...")
            print(f"âŒ CODEX: API call failed: {e}")
            print("ðŸ”„ CODEX: Falling back to simulation...")
            return {
                "success": False,
                "error": str(e),
                "agent": "codex"
            }
    
    async def _simulate_codex(self, prompt: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """Simulate Codex response for testing"""
        await asyncio.sleep(1)  # Simulate API delay
        
        prompt_lower = prompt.lower()
        
        # Generate simulated code based on prompt
        if any(word in prompt_lower for word in ['graphic', 'image', 'draw', 'visual']):
            # Extract what to draw
            subject = "object"
            if 'of' in prompt_lower:
                parts = prompt_lower.split('of', 1)
                if len(parts) > 1:
                    subject = parts[1].strip().split()[0]  # Get first word after "of"
            
            code = f"""<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>{subject.title()} Graphic</title>
    <style>
        body {{
            margin: 0;
            padding: 20px;
            background: #f0f4f8;
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            display: flex;
            justify-content: center;
            align-items: center;
            min-height: 100vh;
        }}
        .container {{
            background: white;
            border-radius: 20px;
            box-shadow: 0 10px 30px rgba(0,0,0,0.1);
            padding: 40px;
            text-align: center;
            max-width: 600px;
        }}
        h1 {{
            color: #2d3748;
            margin-bottom: 30px;
        }}
        svg {{
            filter: drop-shadow(0 4px 6px rgba(0,0,0,0.1));
            transition: transform 0.3s ease;
        }}
        svg:hover {{
            transform: scale(1.05);
        }}
    </style>
</head>
<body>
    <div class="container">
        <h1>{subject.title()} Visualization</h1>
        <svg width="400" height="300" viewBox="0 0 400 300">
            <!-- Placeholder graphic - customize based on subject -->
            <rect x="50" y="50" width="300" height="200" rx="10" fill="#4299e1" />
            <text x="200" y="150" text-anchor="middle" fill="white" font-size="24" font-weight="bold">
                {subject.upper()}
            </text>
        </svg>
    </div>
</body>
</html>"""
        elif "REST API" in prompt:
            code = """
# Generated REST API implementation
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List, Optional
import jwt
from datetime import datetime, timedelta

app = FastAPI()

class User(BaseModel):
    id: Optional[int] = None
    username: str
    email: str
    password_hash: str

class UserCreate(BaseModel):
    username: str
    email: str
    password: str

@app.post("/users/", response_model=User)
async def create_user(user: UserCreate):
    # Hash password and create user
    return User(id=1, username=user.username, email=user.email, password_hash="hashed")

@app.post("/auth/login")
async def login(username: str, password: str):
    # Validate credentials and return JWT
    token = jwt.encode({"user_id": 1}, "secret", algorithm="HS256")
    return {"access_token": token}
"""
        elif "webpage" in prompt.lower() or "html" in prompt.lower():
            # Extract what the webpage should say
            if "Hello Fred" in prompt:
                message = "Hello Fred"
            elif "hello world" in prompt.lower():
                message = "Hello World"
            else:
                message = "Hello World"
            
            code = f"""<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>{message}</title>
    <style>
        body {{
            font-family: Arial, sans-serif;
            display: flex;
            justify-content: center;
            align-items: center;
            min-height: 100vh;
            margin: 0;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
        }}
        .container {{
            text-align: center;
            padding: 2rem;
            border-radius: 15px;
            background: rgba(255, 255, 255, 0.1);
            backdrop-filter: blur(10px);
            box-shadow: 0 8px 32px rgba(0, 0, 0, 0.1);
        }}
        h1 {{
            font-size: 3rem;
            margin: 0;
            text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.3);
        }}
    </style>
</head>
<body>
    <div class="container">
        <h1>{message}</h1>
    </div>
</body>
</html>"""
        else:
            code = f"# Code implementation for: {prompt[:50]}...\n# [Implementation would go here]"
        
        return {
            "success": True,
            "response": code,
            "agent": "codex_simulated",
            "timestamp": datetime.now().isoformat()
        }
    
    def _format_prompt(self, prompt: str, context: Dict[str, Any]) -> str:
        """Format prompt with context for better code generation"""
        # Detect if this is a graphics/visual request
        graphics_keywords = ['graphic', 'image', 'picture', 'draw', 'visual', 'display', 'show']
        is_graphics = any(keyword in prompt.lower() for keyword in graphics_keywords)
        
        if is_graphics and 'web' not in prompt.lower():
            # For graphics, generate web-friendly code
            formatted = f"""Task: {prompt}

IMPORTANT: Generate a web-based solution using HTML/CSS/JavaScript or SVG.
DO NOT use matplotlib, PIL, or other Python graphics libraries.

Create a complete HTML file with embedded CSS and JavaScript that displays the requested graphic.
Use SVG for vector graphics or Canvas API for more complex drawings.

Requirements:
- Self-contained HTML file
- Modern, clean visual design
- Responsive and interactive where appropriate
- Use inline styles and scripts

Generate complete, working code:
"""
        else:
            # Standard code generation
            formatted = f"""Task: {prompt}

Context:
- Project Type: {context.get('project_type', 'general')}
- Language: {context.get('language', 'Python')}
- Framework: {context.get('framework', 'FastAPI')}
- Requirements: {context.get('requirements', [])}

Generate production-ready code with:
- Proper error handling
- Type hints
- Documentation
- Best practices

Code:
"""
        return formatted
    
    def get_capabilities(self) -> List[str]:
        return [
            "code_generation",
            "api_development",
            "database_design",
            "algorithm_implementation",
            "optimization"
        ]


class Claude2Connector(AgentConnector):
    """Connect to Claude2 for validation and quality assurance"""
    
    def __init__(self, project_path: str):
        super().__init__()
        self.project_path = project_path
    
    async def execute(self, prompt: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """Execute Claude2 for validation"""
        self.log("info", "ðŸ” Starting validation task...")
        self.log("info", f"ðŸ“ Validating: {prompt[:100]}...")
        print(f"ðŸ” CLAUDE2: Starting validation task...")
        print(f"ðŸ“ CLAUDE2: Validating: {prompt[:100]}...")
        
        try:
            # Get the implementer's output to analyze actual code
            self.log("info", "ðŸ“‹ Retrieving implementer output for analysis...")
            print("ðŸ“‹ CLAUDE2: Retrieving implementer output for analysis...")
            implementer_output = context.get('implementer_result', {})
            code_content = implementer_output.get('response', '') if implementer_output.get('success') else ''
            
            if code_content:
                self.log("success", f"âœ… Found code to validate ({len(code_content)} chars)")
                self.log("info", f"ðŸ“„ Code preview: {code_content[:150]}...")
                print(f"âœ… CLAUDE2: Found code to validate ({len(code_content)} chars)")
                print(f"ðŸ“„ CLAUDE2: Code preview: {code_content[:150]}...")
            else:
                self.log("warning", "âš ï¸ No code content found in implementer output")
                print("âš ï¸  CLAUDE2: No code content found in implementer output")
            
            # Prepare validation prompt that includes code analysis
            validation_prompt = f"""
As a quality assurance specialist, review and validate the implementation for:

{prompt}

Generated Code:
{code_content}

Please provide:
1. Code quality assessment
2. Security considerations  
3. Performance implications
4. Test recommendations
5. Execution analysis - determine:
   - Is this code runnable? (yes/no)
   - What type of application is it? (web_server, cli_tool, script, etc.)
   - What command should run it?
   - What port does it use (if web app)?
   - What framework/technology?
   - How long does it typically take to start?

Format your response as JSON with these sections:
{{
  "quality_assessment": "...",
  "security_notes": "...", 
  "performance_notes": "...",
  "test_recommendations": "...",
  "execution_metadata": {{
    "runnable": true/false,
    "execution_type": "web_server|cli_tool|script|library",
    "command": "command to run",
    "port": 8000,
    "url": "http://localhost:port",
    "framework": "FastAPI|Flask|Django|etc",
    "startup_time": 2
  }}
}}
"""
            
            print("ðŸ”‘ CLAUDE2: Checking API key...")
            # Use the same SDK approach as main Claude connector
            import anthropic
            
            # Get API key from environment or settings
            api_key = os.getenv("ANTHROPIC_API_KEY")
            if not api_key:
                print("âš ï¸  CLAUDE2: No API key in environment, checking settings...")
                settings_file = Path("settings.json")
                if settings_file.exists():
                    with open(settings_file, "r") as f:
                        settings = json.load(f)
                        api_key = settings.get("anthropic_key")
            
            if not api_key:
                print("âŒ CLAUDE2: No API key found, falling back to simulation...")
                return await self._simulate_validation(prompt, context)
            
            print("âœ… CLAUDE2: API key found, initializing client...")
            client = anthropic.Anthropic(api_key=api_key)
            
            print("ðŸš€ CLAUDE2: Making API call for validation...")
            # Make API call
            message = client.messages.create(
                model="claude-3-sonnet-20240229",
                max_tokens=2000,
                temperature=0,
                messages=[{
                    "role": "user",
                    "content": validation_prompt
                }]
            )
            
            print(f"âœ… CLAUDE2: Validation complete ({len(message.content[0].text)} chars)")
            print(f"ðŸ“„ CLAUDE2: Response preview: {message.content[0].text[:200]}...")
            
            return {
                "success": True,
                "response": message.content[0].text,
                "agent": "claude2_validator",
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            import traceback
            print(f"âŒ CLAUDE2: Validation failed: {e}")
            print(f"âŒ CLAUDE2: Error type: {type(e).__name__}")
            print(f"âŒ CLAUDE2: Full traceback:\n{traceback.format_exc()}")
            print("ðŸ”„ CLAUDE2: Falling back to simulation...")
            # Fallback to simulation
            return await self._simulate_validation(prompt, context)
    
    async def _simulate_validation(self, prompt: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """Simulate validation response"""
        await asyncio.sleep(1)
        
        validation_report = """
## Validation Report

### Code Quality Assessment
- Structure: Well-organized with clear separation of concerns
- Readability: Good use of type hints and docstrings
- Maintainability: Follows SOLID principles

### Security Considerations
- JWT implementation needs secret key management
- Add rate limiting for API endpoints
- Implement input validation for all user inputs

### Performance Implications
- Current design supports ~1000 req/sec
- Consider caching for frequent queries
- Database indexing needed on username field

### Test Recommendations
1. Unit tests for all business logic
2. Integration tests for API endpoints
3. Security testing for authentication
4. Load testing for performance validation

### Improvement Suggestions
- Add comprehensive logging
- Implement health check endpoint
- Consider using environment variables for config
- Add API versioning support
"""
        
        return {
            "success": True,
            "response": validation_report,
            "agent": "claude2_simulated",
            "timestamp": datetime.now().isoformat()
        }
    
    def get_capabilities(self) -> List[str]:
        return [
            "code_review",
            "security_audit",
            "performance_analysis",
            "test_generation",
            "quality_assurance"
        ]


class DeepSeekConnector(AgentConnector):
    """Connect to DeepSeek for advanced reasoning and code analysis"""
    
    def __init__(self, project_path: str):
        super().__init__()
        self.project_path = project_path
        self.api_key = os.getenv("DEEPSEEK_API_KEY")
        self._client = None
    
    def _get_client(self):
        """Lazy load OpenAI client configured for DeepSeek"""
        if not self._client and self.api_key:
            try:
                from openai import OpenAI
                # DeepSeek uses OpenAI-compatible API
                self._client = OpenAI(
                    api_key=self.api_key,
                    base_url="https://api.deepseek.com/v1"
                )
            except ImportError:
                self.log("error", "OpenAI package not installed")
        return self._client
    
    async def execute(self, prompt: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """Execute DeepSeek for advanced reasoning tasks"""
        self.log("info", "ðŸ§  Starting advanced reasoning task...")
        self.log("info", f"ðŸ“ Task: {prompt[:100]}...")
        print(f"ðŸ§  DEEPSEEK: Starting advanced reasoning task...")
        print(f"ðŸ“ DEEPSEEK: Task: {prompt[:100]}...")
        
        client = self._get_client()
        
        if not client:
            self.log("warning", "âš ï¸ No DeepSeek API key, falling back to simulation...")
            print("âš ï¸  DEEPSEEK: No API key found, using simulation...")
            return await self._simulate_deepseek(prompt, context)
        
        try:
            self.log("info", "ðŸš€ Making API call to DeepSeek...")
            print("ðŸš€ DEEPSEEK: Making API call...")
            
            # DeepSeek excels at complex reasoning and code analysis
            system_prompt = """You are an advanced AI reasoning system specialized in:
- Complex algorithmic problem solving
- Deep code analysis and optimization
- Architecture decisions and trade-offs
- Performance bottleneck identification
- Security vulnerability analysis

Provide detailed, thoughtful analysis with concrete recommendations."""
            
            response = await asyncio.to_thread(
                client.chat.completions.create,
                model="deepseek-chat",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": self._format_prompt(prompt, context)}
                ],
                temperature=0.7,
                max_tokens=2000
            )
            
            result_text = response.choices[0].message.content
            
            self.log("success", f"âœ… Analysis complete ({len(result_text)} chars)")
            self.log("info", f"ðŸ“„ Analysis preview: {result_text[:150]}...")
            print(f"âœ… DEEPSEEK: Analysis complete ({len(result_text)} chars)")
            
            return {
                "success": True,
                "response": result_text,
                "agent": "deepseek",
                "model": response.model,
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            self.log("error", f"âŒ API call failed: {e}")
            print(f"âŒ DEEPSEEK: API call failed: {e}")
            return await self._simulate_deepseek(prompt, context)
    
    async def _simulate_deepseek(self, prompt: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """Simulate DeepSeek response for testing"""
        await asyncio.sleep(1.5)
        
        analysis = f"""## Advanced Analysis

### Problem Decomposition
Based on the requirement: {prompt[:100]}...

1. **Core Components Identified**:
   - Primary functionality requires modular architecture
   - Performance considerations suggest async implementation
   - Security implications require input validation

2. **Algorithmic Approach**:
   - Time complexity: O(n log n) for main operations
   - Space complexity: O(n) with potential for optimization
   - Suggested data structures: HashMap for O(1) lookups

3. **Architecture Recommendations**:
   - Microservices pattern for scalability
   - Event-driven architecture for real-time features
   - CQRS pattern for read/write optimization

4. **Performance Optimizations**:
   - Implement caching layer (Redis recommended)
   - Use database indexing on frequent query fields
   - Consider CDN for static assets

5. **Security Considerations**:
   - Implement rate limiting
   - Use JWT with refresh tokens
   - Add input sanitization middleware

### Trade-off Analysis
- **Complexity vs Performance**: Current design favors performance
- **Flexibility vs Simplicity**: Modular approach allows future extensions
- **Cost vs Scalability**: Horizontal scaling more cost-effective long-term

### Recommended Next Steps
1. Implement proof of concept for core algorithm
2. Benchmark performance with realistic data
3. Security audit before production deployment
"""
        
        return {
            "success": True,
            "response": analysis,
            "agent": "deepseek_simulated",
            "timestamp": datetime.now().isoformat()
        }
    
    def _format_prompt(self, prompt: str, context: Dict[str, Any]) -> str:
        """Format prompt with context for DeepSeek analysis"""
        formatted = f"""Analyze this requirement with deep technical insight:

{prompt}

Context:
- Project Type: {context.get('project_type', 'general')}
- Existing Architecture: {context.get('architecture', 'Not specified')}
- Performance Requirements: {context.get('performance', 'Standard')}
- Security Level: {context.get('security', 'Standard')}

Previous Analysis (if any):
{context.get('analyst_result', {}).get('response', 'None')}

Previous Implementation (if any):
{context.get('implementer_result', {}).get('response', 'None')[:500]}...

Provide:
1. Detailed technical analysis
2. Algorithm recommendations
3. Performance optimization strategies
4. Security considerations
5. Architecture trade-offs
6. Scalability analysis
"""
        return formatted
    
    def get_capabilities(self) -> List[str]:
        return [
            "deep_reasoning",
            "algorithm_analysis", 
            "performance_optimization",
            "security_analysis",
            "architecture_review",
            "code_quality_assessment",
            "complexity_analysis"
        ]


class IDEConnector(AgentConnector):
    """Connect to IDE tools for deployment and integration"""
    
    def __init__(self, ide_type: str = "vscode"):
        super().__init__()
        self.ide_type = ide_type
        self.supported_ides = ["vscode", "cursor", "windsurf"]
    
    async def execute(self, prompt: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """Execute IDE automation tasks"""
        try:
            if self.ide_type not in self.supported_ides:
                return {
                    "success": False,
                    "error": f"Unsupported IDE: {self.ide_type}"
                }
            
            # Simulate IDE operations
            operation = context.get("operation", "deploy")
            
            if operation == "deploy":
                result = await self._simulate_deployment()
            elif operation == "test":
                result = await self._simulate_testing()
            else:
                result = {"status": "completed", "operation": operation}
            
            return {
                "success": True,
                "response": result,
                "agent": f"{self.ide_type}_ide",
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "agent": f"{self.ide_type}_ide"
            }
    
    async def _simulate_deployment(self) -> Dict[str, Any]:
        """Simulate deployment process"""
        await asyncio.sleep(2)
        
        return {
            "deployment_status": "success",
            "environment": "staging",
            "url": "https://api-staging.example.com",
            "health_check": "passing",
            "metrics": {
                "response_time": "45ms",
                "uptime": "99.9%"
            }
        }
    
    async def _simulate_testing(self) -> Dict[str, Any]:
        """Simulate test execution"""
        await asyncio.sleep(1.5)
        
        return {
            "test_results": {
                "total": 42,
                "passed": 40,
                "failed": 2,
                "coverage": "87%"
            },
            "failed_tests": [
                "test_user_creation_duplicate_email",
                "test_jwt_expiration"
            ]
        }
    
    def get_capabilities(self) -> List[str]:
        return [
            "deployment",
            "testing",
            "debugging",
            "environment_setup",
            "monitoring"
        ]


class AgentBridge:
    """
    Main bridge connecting Orchestrator to all agents
    Manages agent lifecycle and task routing
    """
    
    def __init__(self, project_path: str):
        self.project_path = project_path
        self.agents = {}
        
        # Initialize all connectors
        self._initialize_connectors()
    
    def _initialize_connectors(self):
        """Initialize all agent connectors"""
        # Analysis agent (Claude)
        self.agents["claude_analyst"] = ClaudeConnector(self.project_path)
        
        # Implementation agent (Codex)
        self.agents["codex_impl"] = CodexConnector()
        
        # Validation agent (Claude2)
        self.agents["claude2_validator"] = Claude2Connector(self.project_path)
        
        # Deep reasoning agent (DeepSeek)
        self.agents["deepseek_reasoner"] = DeepSeekConnector(self.project_path)
        
        # IDE agents
        self.agents["vscode_ide"] = IDEConnector("vscode")
        self.agents["cursor_ide"] = IDEConnector("cursor")
    
    async def execute_task(self, agent_id: str, task: Dict[str, Any]) -> Dict[str, Any]:
        """Route task to appropriate agent and return results"""
        if agent_id not in self.agents:
            return {
                "success": False,
                "error": f"Unknown agent: {agent_id}"
            }
        
        agent = self.agents[agent_id]
        
        # Execute with agent
        result = await agent.execute(
            prompt=task.get("description", ""),
            context=task.get("context", {})
        )
        
        # Add task metadata to result
        result["task_id"] = task.get("id")
        result["agent_id"] = agent_id
        
        return result
    
    def get_agent_capabilities(self, agent_id: str) -> List[str]:
        """Get capabilities of a specific agent"""
        if agent_id in self.agents:
            return self.agents[agent_id].get_capabilities()
        return []
    
    def get_all_agents(self) -> Dict[str, List[str]]:
        """Get all agents and their capabilities"""
        return {
            agent_id: agent.get_capabilities()
            for agent_id, agent in self.agents.items()
        }


# Example usage
if __name__ == "__main__":
    async def test_bridge():
        bridge = AgentBridge("/tmp/test_project")
        
        # Test Claude analysis
        result = await bridge.execute_task("claude_analyst", {
            "id": "test-1",
            "description": "Design architecture for user management API",
            "context": {"project_type": "web_api"}
        })
        print(f"Claude result: {json.dumps(result, indent=2)}")
        
        # Test Codex implementation
        result = await bridge.execute_task("codex_impl", {
            "id": "test-2",
            "description": "Implement user registration endpoint",
            "context": {"framework": "FastAPI", "language": "Python"}
        })
        print(f"Codex result: {result['success']}")
    
    asyncio.run(test_bridge())